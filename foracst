import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import xgboost as xgb
import matplotlib.pyplot as plt
# Example Dataset
data_start = pd.DataFrame({
    'Month': pd.date_range(start='2020-01-01', periods=36, freq='ME'),
    'Sales': np.random.randint(200, 500, 36)
})
data = data_start.copy()
# Create Features (e.g., Lagged Sales and Time Index)
data['MonthIndex'] = np.arange(len(data))  # Numeric time index
data['Lag1'] = data['Sales'].shift(1)     # Lag feature
data['Lag2'] = data['Sales'].shift(2)

# Drop rows with NaN due to lagging
data = data.dropna()

# Add 'Month' column to features
X = data[['MonthIndex', 'Lag1', 'Lag2']]
y = data['Sales']

# Keep track of the corresponding months
data['Month'] = data['Month']

# Split into train and test
X_train, X_test, y_train, y_test, months_train, months_test = train_test_split(
    X, y, data['Month'], test_size=0.2, random_state=42
)
# Train the Linear Regression model
lr_model.fit(X_train, y_train)

# Predict with Linear Regression
y_pred = lr_model.predict(X_test)

# Prepare XGBoost datasets
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

# Train the XGBoost model
params = {'objective': 'reg:squarederror', 'max_depth': 3, 'learning_rate': 0.1}
xgb_model = xgb.train(params, dtrain, num_boost_round=100)

# Predict with XGBoost
y_pred_xgb = xgb_model.predict(dtest)

# Combine results into a DataFrame with real months
results = pd.DataFrame({
    'Month': months_test.values,  # Use real months from the test set
    'Actual Sales': y_test.values,
    'Predicted (LinearRegression)': y_pred,
    'Predicted (XGBoost)': y_pred_xgb
}).sort_values(by='Month')

# Reset the index to align with Month
results = results.reset_index(drop=True)

# Display the table
print(results)
print("\nXGBoost:")
print("R2 Score:", r2_score(y_test, y_pred_xgb))
print("RMSE:", np.sqrt(mean_squared_error(y_test, y_pred_xgb)))
print("Linear Regression:")
print("R2 Score:", r2_score(y_test, y_pred))
print("RMSE:", np.sqrt(mean_squared_error(y_test, y_pred)))

formatted_months = pd.to_datetime(months_test.values).strftime('%Y-%m-%d')

# Update the plot with formatted x-axis labels
plt.figure(figsize=(10, 6))

# Plot the actual and predicted values
plt.plot(y_test.values, label='Actual', marker='o')
plt.plot(y_pred, label='LR Predicted', linestyle='dashed')
plt.plot(y_pred_xgb, label='XGBoost Predicted', linestyle='dotted')

# Add the x-axis labels with formatted dates
plt.xticks(
    ticks=range(len(months_test)),  # Positions for the ticks
    labels=formatted_months,  # Formatted date labels
    rotation=45  # Rotate for readability
)

# Add title, legend, and axis labels
plt.title('Actual vs Predicted Sales')
plt.xlabel('Months')
plt.ylabel('Sales')
plt.legend()
plt.tight_layout()  # Ensure everything fits nicely
plt.show()
# Generate future months
last_month = data['Month'].max()
future_months = pd.date_range(start=last_month, periods=12, freq='ME')  # Forecast for 12 future months

# Prepare future features using the most recent data
future_data = pd.DataFrame({
    'Month': future_months,
    'MonthIndex': np.arange(len(data), len(data) + len(future_months)),
    'Lag1': data['Sales'].iloc[-1],  # Last known sales value
    'Lag2': data['Sales'].iloc[-2]   # Second-to-last known sales value
})

# Predict future sales
future_features = future_data[['MonthIndex', 'Lag1', 'Lag2']]
future_data['Predicted (LinearRegression)'] = lr_model.predict(future_features)
future_data['Predicted (XGBoost)'] = xgb_model.predict(xgb.DMatrix(future_features))

# Combine past and future into one table
all_results = pd.concat([data_start, future_data[['Month', 'Predicted (LinearRegression)', 'Predicted (XGBoost)']]])
all_results = pd.concat([all_results, results[['Month', 'Predicted (LinearRegression)', 'Predicted (XGBoost)']]])

print(all_results)
